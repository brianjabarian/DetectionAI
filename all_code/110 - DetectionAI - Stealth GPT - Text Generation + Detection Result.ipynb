{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate AI Texts  from Previous AI-generated Texts using Stealth GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4206214,
     "status": "error",
     "timestamp": 1747683598082,
     "user": {
      "displayName": "Ziyue Feng",
      "userId": "14163341396518175432"
     },
     "user_tz": 300
    },
    "id": "-autgX3lS2PI",
    "outputId": "f4d9c211-00e9-4112-89bc-d986fb829b6d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from getpass import getpass\n",
    "\n",
    "# ======== Configure paths and token ========\n",
    "input_file = '' # JSON FILE FROM AI-GENERATED TEXT\n",
    "output_file = ''\n",
    "\n",
    "# Securely prompt for the API token\n",
    "api_token = getpass(\"Please enter your StealthGPT API token: \")\n",
    "if not api_token:\n",
    "    raise ValueError(\"A StealthGPT API token is required to run this script.\")\n",
    "\n",
    "API_URL = 'https://stealthgpt.ai/api/stealthify'\n",
    "MAX_PROMPT_LENGTH = 3000\n",
    "\n",
    "# ======== Load existing output data (if it exists) ========\n",
    "if os.path.exists(output_file):\n",
    "    with open(output_file, 'r', encoding='utf-8') as f:\n",
    "        # Create a dictionary lookup for entries that have already been processed\n",
    "        processed_lookup = {entry.get('topic'): entry for entry in json.load(f)}\n",
    "    print(f\"Loaded {len(processed_lookup)} entries from existing output file.\")\n",
    "else:\n",
    "    processed_lookup = {}\n",
    "\n",
    "# ======== Load the main input data ========\n",
    "with open(input_file, 'r') as f:\n",
    "    input_data = json.load(f)\n",
    "\n",
    "# The final list of data we will save\n",
    "final_data = list(processed_lookup.values())\n",
    "\n",
    "# ======== Process entries one by one ========\n",
    "for entry in tqdm(input_data, desc=\"Processing and rephrasing entries\"):\n",
    "    topic = entry.get('topic')\n",
    "    \n",
    "    # Decide whether to process this entry as new or as an update\n",
    "    if topic in processed_lookup:\n",
    "        # --- UPDATE LOGIC: Entry exists, check for failed 'None' values ---\n",
    "        existing_entry = processed_lookup[topic]\n",
    "        rephrased_texts = existing_entry.get(\"stealthgpt_rephrased\", {})\n",
    "        was_updated = False\n",
    "        \n",
    "        for model_name, rephrased_text in rephrased_texts.items():\n",
    "            if rephrased_text is None:\n",
    "                # This specific model failed before, so we retry it\n",
    "                original_text = entry.get(\"ai_generated\", {}).get(model_name)\n",
    "                if not original_text: continue\n",
    "\n",
    "                # (API call logic is duplicated here for clarity)\n",
    "                if len(original_text) > MAX_PROMPT_LENGTH:\n",
    "                    original_text = original_text[:MAX_PROMPT_LENGTH]\n",
    "                payload = {\"prompt\": original_text, \"rephrase\": True, \"tone\": \"Standard\", \"mode\": \"Medium\"}\n",
    "                headers = {\"api-token\": api_token, \"Content-Type\": \"application/json\"}\n",
    "                \n",
    "                try:\n",
    "                    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "                    response.raise_for_status()\n",
    "                    result = response.json().get(\"result\", \"\")\n",
    "                    existing_entry[\"stealthgpt_rephrased\"][model_name] = result\n",
    "                    was_updated = True\n",
    "                    print(f\"\\n‚úÖ Retried and rephrased '{model_name}' for topic: {topic[:40]}...\")\n",
    "                except requests.RequestException as e:\n",
    "                    print(f\"\\n‚ùå Retry failed for '{model_name}' on topic: {topic[:40]}...: {e}\")\n",
    "        \n",
    "        if was_updated:\n",
    "            # If we made an update, save progress immediately\n",
    "            with open(output_file, 'w') as f:\n",
    "                json.dump(final_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    else:\n",
    "        # --- NEW ENTRY LOGIC: Process this entry for the first time ---\n",
    "        ai_texts = entry.get(\"ai_generated\", {})\n",
    "        if not ai_texts: continue\n",
    "\n",
    "        entry.setdefault(\"stealthgpt_rephrased\", {})\n",
    "        \n",
    "        for model_name, original_text in ai_texts.items():\n",
    "            if not original_text: continue\n",
    "\n",
    "            if len(original_text) > MAX_PROMPT_LENGTH:\n",
    "                original_text = original_text[:MAX_PROMPT_LENGTH]\n",
    "            payload = {\"prompt\": original_text, \"rephrase\": True, \"tone\": \"Standard\", \"mode\": \"Medium\"}\n",
    "            headers = {\"api-token\": api_token, \"Content-Type\": \"application/json\"}\n",
    "\n",
    "            try:\n",
    "                response = requests.post(API_URL, headers=headers, json=payload)\n",
    "                response.raise_for_status()\n",
    "                result = response.json().get(\"result\", \"\")\n",
    "                entry[\"stealthgpt_rephrased\"][model_name] = result\n",
    "                print(f\"\\n‚úÖ Processed '{model_name}' for new topic: {topic[:40]}...\")\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"\\n‚ùå Failed to process '{model_name}' for new topic: {topic[:40]}...: {e}\")\n",
    "                entry[\"stealthgpt_rephrased\"][model_name] = None\n",
    "        \n",
    "        # Add the newly processed entry to our data and save\n",
    "        final_data.append(entry)\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(final_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ All done! Final results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Five AI Detectors (4 API and 1 fine-tuned model) to Detect Stealth GPT Texts and Provide Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2203509,
     "status": "ok",
     "timestamp": 1747690630106,
     "user": {
      "displayName": "Ziyue Feng",
      "userId": "14163341396518175432"
     },
     "user_tz": 300
    },
    "id": "mmydPLJfOlvL",
    "outputId": "e0d99ef0-5b22-4fe3-8f57-34ab115e00b3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Pengram API Key:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
      "Enter Originality.AI API Key:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
      "Enter GPTZero API Key:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API keys loaded for this session.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Running all AI detectors:   3%|‚ñé         | 50/1992 [05:10<4:38:37,  8.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (50 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors:   5%|‚ñå         | 100/1992 [11:22<3:40:20,  6.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (100 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors:   8%|‚ñä         | 150/1992 [16:19<2:24:31,  4.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (150 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors:  10%|‚ñà         | 200/1992 [19:55<2:21:49,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (200 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors:  13%|‚ñà‚ñé        | 250/1992 [23:38<2:35:31,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (250 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors:  15%|‚ñà‚ñå        | 300/1992 [27:12<2:19:00,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (300 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors:  18%|‚ñà‚ñä        | 350/1992 [30:26<1:58:07,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (350 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors:  20%|‚ñà‚ñà        | 400/1992 [33:52<1:43:11,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (400 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors:  23%|‚ñà‚ñà‚ñé       | 450/1992 [39:29<3:12:11,  7.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (450 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors:  25%|‚ñà‚ñà‚ñå       | 500/1992 [45:25<3:04:58,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (500 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors:  28%|‚ñà‚ñà‚ñä       | 550/1992 [48:35<1:25:09,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (550 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors:  30%|‚ñà‚ñà‚ñà       | 600/1992 [52:01<1:41:48,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (600 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors:  33%|‚ñà‚ñà‚ñà‚ñé      | 650/1992 [57:55<2:36:47,  7.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (650 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors:  35%|‚ñà‚ñà‚ñà‚ñå      | 700/1992 [1:03:15<1:37:37,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (700 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors:  38%|‚ñà‚ñà‚ñà‚ñä      | 750/1992 [1:08:09<1:39:28,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (750 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors:  40%|‚ñà‚ñà‚ñà‚ñà      | 800/1992 [1:12:39<2:40:14,  8.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (800 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 850/1992 [1:17:25<1:26:15,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (850 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 900/1992 [1:21:51<1:45:28,  5.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (900 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 950/1992 [1:26:32<2:14:22,  7.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (950 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1000/1992 [1:31:46<1:52:39,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (1000 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1050/1992 [1:38:12<1:50:04,  7.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (1050 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1100/1992 [1:47:47<3:05:29, 12.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (1100 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1150/1992 [1:55:35<1:58:07,  8.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (1150 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1200/1992 [2:03:09<1:45:39,  8.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (1200 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1250/1992 [2:10:05<1:38:47,  7.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (1250 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 1300/1992 [2:18:13<2:11:22, 11.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (1300 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 1350/1992 [2:30:23<2:44:58, 15.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (1350 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 1400/1992 [2:41:04<2:56:00, 17.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (1400 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 1450/1992 [2:49:59<1:37:19, 10.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (1450 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 1500/1992 [2:57:26<48:26,  5.91s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (1500 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 1550/1992 [3:04:40<1:12:51,  9.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (1550 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1600/1992 [3:12:19<1:09:27, 10.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (1600 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 1650/1992 [3:18:56<33:30,  5.88s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (1650 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 1700/1992 [3:23:58<34:41,  7.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (1700 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 1750/1992 [3:29:01<24:29,  6.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (1750 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 1800/1992 [3:34:02<18:14,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (1800 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 1850/1992 [3:38:51<13:48,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (1850 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 1900/1992 [3:43:40<09:05,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (1900 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1950/1992 [3:48:45<04:14,  6.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved! (1950 items processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running all AI detectors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1992/1992 [3:52:57<00:00,  7.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Multi-detector analysis completed.\n",
      "Total items: 1992\n",
      "Skipped items (already processed): 0\n",
      "Processed items: 1992\n",
      "Results saved to: generated_output_claude-opus-4-20250514_stealthgpt_all_detectors.json\n"
     ]
    }
   ],
   "source": [
    "# merge all ai-detector, and remain default prob\n",
    "# Multi-detector AI text analysis with default prob\n",
    "import json\n",
    "import requests\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "from getpass import getpass\n",
    "\n",
    "# === Prompt for API keys securely ===\n",
    "api_keys = {\n",
    "    \"pengram\": getpass(\"Enter Pengram API Key: \"),\n",
    "    \"originality\": getpass(\"Enter Originality.AI API Key: \"),\n",
    "    \"gptzero\": getpass(\"Enter GPTZero API Key: \")\n",
    "}\n",
    "\n",
    "print(\"‚úÖ API keys loaded for this session.\")\n",
    "\n",
    "# === Pangram Ê£ÄÊµãÂáΩÊï∞ ===\n",
    "def detect_ai_pangram(text):\n",
    "    if not text.strip():\n",
    "        return None\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"x-api-key\": api_keys[\"pengram\"]\n",
    "    }\n",
    "    payload = {\"text\": text}\n",
    "    try:\n",
    "        response = requests.post(\"https://text.api.pangramlabs.com\", headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "\n",
    "        likelihood = result.get(\"ai_likelihood\")\n",
    "        prediction = result.get(\"prediction\")\n",
    "\n",
    "        return {\n",
    "            \"ai_likelihood\": likelihood,\n",
    "            \"prediction\": prediction\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# === Originality.ai wrapper ===\n",
    "def detect_ai_originality(text):\n",
    "    if not text.strip():\n",
    "        return None\n",
    "    try:\n",
    "        headers = {\n",
    "            \"X-OAI-API-KEY\": api_keys[\"originality\"],\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        payload = {\n",
    "            \"check_ai\": True,\n",
    "            \"check_plagiarism\": False,\n",
    "            \"check_facts\": False,\n",
    "            \"check_readability\": False,\n",
    "\n",
    "            \"check_grammar\": False,\n",
    "            \"check_contentOptimizer\": False,\n",
    "            \"storeScan\": False,\n",
    "            \"aiModelVersion\": \"lite\",\n",
    "            \"content\": text\n",
    "        }\n",
    "        response = requests.post(\"https://api.originality.ai/api/v3/scan\", headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        classification = result.get(\"results\", {}).get(\"ai\", {}).get(\"classification\", {})\n",
    "        confidence = result.get(\"results\", {}).get(\"ai\", {}).get(\"confidence\", {})\n",
    "        return {\n",
    "            \"classification\": {\n",
    "                \"AI\": classification.get(\"AI\"),\n",
    "                \"Original\": classification.get(\"Original\")\n",
    "            },\n",
    "            \"confidence\": {\n",
    "                \"AI\": confidence.get(\"AI\"),\n",
    "                \"Original\": confidence.get(\"Original\")\n",
    "            }\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# === GPTZero detector wrapper ===\n",
    "def detect_ai_gptzero(text):\n",
    "    if not text.strip():\n",
    "        return None\n",
    "    try:\n",
    "        headers = {\n",
    "            \"Accept\": \"application/json\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"x-api-key\": api_keys[\"gptzero\"]\n",
    "        }\n",
    "        payload = {\n",
    "            \"document\": text,\n",
    "            \"multilingual\": False\n",
    "        }\n",
    "        response = requests.post(\n",
    "            \"https://api.gptzero.me/v2/predict/text\",\n",
    "            headers=headers,\n",
    "            json=payload\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        doc = response.json().get(\"documents\", [{}])[0]\n",
    "        return {\n",
    "            \"average_generated_prob\": doc.get(\"average_generated_prob\")\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# === Load RoBERTa-based OpenAI Detector ===\n",
    "MODEL_NAME = \"roberta-base-openai-detector\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "detector = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME).eval()\n",
    "\n",
    "# === RoBERTa detector function ===\n",
    "def detect_ai_roberta(text):\n",
    "    if not text.strip():\n",
    "        return None\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        logits = detector(**inputs).logits\n",
    "    prob_ai = torch.softmax(logits, dim=-1)[0, 1].item()\n",
    "    return prob_ai\n",
    "\n",
    "# === Load input data ===\n",
    "input_path = \"\" ## JSON FILE FROM STEALTH GPT GENERATED TEXT\n",
    "# --- NEW: Define the output path once at the top ---\n",
    "output_path = input_path.replace(\".json\", \"_all_detectors.json\")\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# === NEW: Define how often to save the file (e.g., every 50 items) ===\n",
    "SAVE_INTERVAL = 50\n",
    "\n",
    "# ÁªüËÆ°‰ø°ÊÅØ\n",
    "total_items = len(data)\n",
    "skipped_items = 0\n",
    "processed_items = 0\n",
    "\n",
    "# === Run all detectors ===\n",
    "for item in tqdm(data, desc=\"Running all AI detectors\"):\n",
    "    # Ê£ÄÊü•ÊòØÂê¶Â∑≤ÊúâÊ£ÄÊµãÁªìÊûúÔºåÂ¶ÇÊûúÊúâÂàôË∑≥Ëøá\n",
    "    if \"human_verdict\" in item and \"ai_verdicts\" in item and all(model in item[\"ai_verdicts\"] for model in item.get(\"stealthgpt_rephrased\", {})):\n",
    "        skipped_items += 1\n",
    "        continue\n",
    "\n",
    "    human_text = item.get(\"text\", \"\")\n",
    "\n",
    "    # Initialize verdict structure if needed\n",
    "    item.setdefault(\"human_verdict\", {})\n",
    "\n",
    "    # Add results from each detector\n",
    "    item[\"human_verdict\"][\"pengram\"] = detect_ai_pangram(human_text)\n",
    "    item[\"human_verdict\"][\"originality\"] = detect_ai_originality(human_text)\n",
    "    item[\"human_verdict\"][\"gptzero\"] = detect_ai_gptzero(human_text)\n",
    "    item[\"human_verdict\"][\"roberta-base-detector\"] = detect_ai_roberta(human_text)\n",
    "\n",
    "    # Process AI-generated texts\n",
    "    ai_texts = item.get(\"stealthgpt_rephrased\", {})\n",
    "    item.setdefault(\"ai_verdicts\", {})\n",
    "\n",
    "    for model_name, text in ai_texts.items():\n",
    "        if not text:\n",
    "            continue\n",
    "        item[\"ai_verdicts\"].setdefault(model_name, {})\n",
    "        item[\"ai_verdicts\"][model_name][\"pengram\"] = detect_ai_pangram(text)\n",
    "        item[\"ai_verdicts\"][model_name][\"originality\"] = detect_ai_originality(text)\n",
    "        item[\"ai_verdicts\"][model_name][\"gptzero\"] = detect_ai_gptzero(text)\n",
    "        item[\"ai_verdicts\"][model_name][\"roberta-base-detector\"] = detect_ai_roberta(text)\n",
    "\n",
    "    processed_items += 1\n",
    "\n",
    "    # --- NEW: Automatically save progress at the specified interval ---\n",
    "    if processed_items > 0 and processed_items % SAVE_INTERVAL == 0:\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        # Using tqdm.write is better than print() here as it doesn't break the progress bar\n",
    "        tqdm.write(f\"üíæ Progress saved! ({processed_items} items processed)\")\n",
    "\n",
    "\n",
    "# === Final Save Output ===\n",
    "# This final save ensures that any remaining items are saved after the loop finishes.\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Multi-detector analysis completed.\")\n",
    "print(f\"Total items: {total_items}\")\n",
    "print(f\"Skipped items (already processed): {skipped_items}\")\n",
    "print(f\"Processed items: {processed_items}\")\n",
    "print(f\"Results saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNDXRDZuU57ZJmFKqq5uJOY",
   "mount_file_id": "1YWrGKSRraQnGxvSYSJJlRPKtYBH04r-i",
   "provenance": []
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
