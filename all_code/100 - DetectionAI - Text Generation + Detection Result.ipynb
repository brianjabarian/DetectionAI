{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f947fc3-6f75-4e85-8bf5-b501ca28c65e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84f041b-c20f-48a2-b973-e6b78eb8469a",
   "metadata": {},
   "source": [
    "# Generate Topics from Human Text for Various LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb94e54a-d09c-4583-9561-f21a2a05f805",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb3cad7a-af21-496f-9faf-df6f8265c0f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please choose a model to use:\n",
      "1: GPT-4.1 (openai)\n",
      "2: GPT o3 (openai)\n",
      "3: Claude Opus 4 (anthropic)\n",
      "4: Claude Sonnet 4 (anthropic)\n",
      "5: Gemini 2.5 Flash (google)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number of your choice:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have selected: GPT-4.1\n"
     ]
    }
   ],
   "source": [
    "# --- Model and API Key Selection ---\n",
    "\n",
    "# Added Google's Gemini models, specifying the \"google\" provider\n",
    "MODELS = {\n",
    "    \"1\": {\"name\": \"GPT-4.1\", \"id\": \"gpt-4.1-2025-04-14\", \"provider\": \"openai\"},\n",
    "    \"2\": {\"name\": \"GPT o3\", \"id\": \"o3-2025-04-16\", \"provider\": \"openai\"},\n",
    "    \"3\": {\"name\": \"Claude Opus 4\", \"id\": \"claude-opus-4-20250514\", \"provider\": \"anthropic\"},\n",
    "    \"4\": {\"name\": \"Claude Sonnet 4\", \"id\": \"claude-sonnet-4-20250514\", \"provider\": \"anthropic\"},\n",
    "    \"5\": {\"name\": \"Gemini 2.0 Flash\", \"id\": \"gemini-2.0-flash\", \"provider\": \"google\"},\n",
    "}\n",
    "\n",
    "# Prompt user to select a model\n",
    "print(\"Please choose a model to use:\")\n",
    "for key, model_info in sorted(MODELS.items()):\n",
    "    print(f\"{key}: {model_info['name']} ({model_info['provider']})\")\n",
    "\n",
    "choice = input(\"Enter the number of your choice: \")\n",
    "selected_model = MODELS.get(choice)\n",
    "\n",
    "if not selected_model:\n",
    "    raise ValueError(\"Invalid choice. Please run the script again.\")\n",
    "\n",
    "print(f\"You have selected: {selected_model['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7cdce3bf-7ca9-41ba-ab76-c1180d9217f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your OpenAI API key:  ········\n"
     ]
    }
   ],
   "source": [
    "# --- Client and Authentication Setup ---\n",
    "PROJECT_ID = \"\" ## GCP PROJECT\n",
    "LOCATION = \"\" ## GCP REGION/LOCATION  \n",
    "\n",
    "# This block now handles all three providers\n",
    "client = None\n",
    "provider = selected_model[\"provider\"]\n",
    "\n",
    "if provider == \"openai\":\n",
    "    from openai import OpenAI\n",
    "    api_key = getpass(\"Please enter your OpenAI API key: \")\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "elif provider == \"anthropic\":\n",
    "    from anthropic import Anthropic\n",
    "    api_key = getpass(\"Please enter your Anthropic API key: \")\n",
    "    client = Anthropic(api_key=api_key)\n",
    "\n",
    "elif provider == \"google\":\n",
    "    # Key-less authentication for Vertex AI\n",
    "    print(\"Authenticating to Google Cloud via Vertex AI environment...\")\n",
    "    try:\n",
    "        from vertexai.preview.generative_models import GenerativeModel, Part, HarmCategory, HarmBlockThreshold\n",
    "        import vertexai\n",
    "        \n",
    "        # You must specify your Google Cloud project ID and location\n",
    "        PROJECT_ID = PROJECT_ID\n",
    "        LOCATION = LOCATION\n",
    "\n",
    "        vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "        # Instead of a 'client', we instantiate the specific model\n",
    "        client = GenerativeModel(selected_model['id'])\n",
    "        print(\"Successfully initialized Gemini model.\")\n",
    "    except ImportError:\n",
    "        print(\"ERROR: 'google-cloud-aiplatform' library not found.\")\n",
    "        print(\"Please run: pip install google-cloud-aiplatform\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not initialize Vertex AI. Your project ID may be incorrect or you may not be in a Vertex AI notebook.\")\n",
    "        print(f\"Details: {e}\")\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78e387f2-d9ef-4ec2-bf5f-cb4cf2b29cfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Functions ---\n",
    "\n",
    "def get_prompt(text):\n",
    "    return f\"\"\"You are a topic summarizer.\n",
    "Summarize the following paragraph into a very short topic (max 40 words).\n",
    "Avoid using full sentences. Be specific.\n",
    "Text:\n",
    "{text}\n",
    "Return only the topic without any explanation.\"\"\"\n",
    "\n",
    "def generate_topic(text, model_id, provider):\n",
    "    \"\"\"\n",
    "    Generates a topic using the dynamically selected model and provider.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if provider == \"openai\":\n",
    "            response = client.chat.completions.create(\n",
    "                model=model_id,\n",
    "                messages=[{\"role\": \"user\", \"content\": get_prompt(text)}],\n",
    "                temperature=0.3, max_tokens=40\n",
    "            )\n",
    "            return response.choices[0].message.content.strip().strip('\"')\n",
    "        \n",
    "        elif provider == \"anthropic\":\n",
    "            response = client.messages.create(\n",
    "                model=model_id,\n",
    "                system=\"You are a topic summarizer. Summarize the following paragraph into a very short topic (max 40 words). Avoid using full sentences. Be specific. Return only the topic without any explanation.\",\n",
    "                messages=[{\"role\": \"user\", \"content\": f\"Text:\\n{text}\"}],\n",
    "                temperature=0.3, max_tokens=40\n",
    "            )\n",
    "            return response.content[0].text.strip().strip('\"')\n",
    "\n",
    "        elif provider == \"google\":\n",
    "            # The 'client' here is actually the instantiated Gemini model\n",
    "            prompt = get_prompt(text)\n",
    "            response = client.generate_content(\n",
    "                [Part.from_text(prompt)],\n",
    "                generation_config={\"temperature\": 0.3, \"max_output_tokens\": 40},\n",
    "                # Safety settings to reduce chances of blocking for benign content\n",
    "                safety_settings={\n",
    "                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "                }\n",
    "            )\n",
    "            return response.text.strip().strip('\"')\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating topic with {model_id}: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac67a6f9-a7b6-4e2d-a081-bab9596ef351",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating topics with GPT-4.1: 100%|██████████| 2/2 [00:04<00:00,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All topics have been generated and saved to topics_gpt-4.1-2025-04-14_20250703.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Main Execution Logic ---\n",
    "\n",
    "## PROVIDE RAW DATA\n",
    "\n",
    "# input_path = \"../original_corpus/original_corpus.json\"\n",
    "input_path = \"\"\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
    "output_path = f\"topics_{selected_model['id'].replace('/', '_')}_{timestamp}.json\"\n",
    "\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for item in tqdm(data, desc=f\"Generating topics with {selected_model['name']}\"):\n",
    "    if not item.get(\"topic\"):\n",
    "        item[\"topic\"] = generate_topic(item[\"text\"], selected_model[\"id\"], provider)\n",
    "        time.sleep(1) \n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"All topics have been generated and saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727602fa-9a55-42e0-a312-90f2a31f66d3",
   "metadata": {},
   "source": [
    "# Generate AI Texts using Various LLMs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7baad2f-a796-4a50-b4fa-751577672cb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please choose a model to use for generation:\n",
      "1: GPT-4.1 (openai)\n",
      "2: GPT o3 (openai)\n",
      "3: Claude Opus 4 (anthropic)\n",
      "4: Claude Sonnet 4 (anthropic)\n",
      "5: Gemini 2.5 Flash (google)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number of your choice:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have selected: GPT-4.1\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your OpenAI API key:  ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from getpass import getpass\n",
    "\n",
    "# --- 1. Model and Provider Configuration ---\n",
    "# A central dictionary to manage all models and their providers.\n",
    "MODELS = {\n",
    "    \"1\": {\"name\": \"GPT-4.1\", \"id\": \"gpt-4.1-2025-04-14\", \"provider\": \"openai\"},\n",
    "    \"2\": {\"name\": \"GPT o3\", \"id\": \"o3-2025-04-16\", \"provider\": \"openai\"},\n",
    "    \"3\": {\"name\": \"Claude Opus 4\", \"id\": \"claude-opus-4-20250514\", \"provider\": \"anthropic\"},\n",
    "    \"4\": {\"name\": \"Claude Sonnet 4\", \"id\": \"claude-sonnet-4-20250514\", \"provider\": \"anthropic\"},\n",
    "    \"5\": {\"name\": \"Gemini 2.0 Flash\", \"id\": \"gemini-2.0-flash\", \"provider\": \"google\"}\n",
    "}\n",
    "\n",
    "# --- 2. Dynamic Model and Authentication Setup ---\n",
    "print(\"Please choose a model to use for generation:\")\n",
    "for key, model_info in sorted(MODELS.items()):\n",
    "    print(f\"{key}: {model_info['name']} ({model_info['provider']})\")\n",
    "\n",
    "choice = input(\"Enter the number of your choice: \")\n",
    "selected_model = MODELS.get(choice)\n",
    "\n",
    "if not selected_model:\n",
    "    raise ValueError(\"Invalid choice. Please run the script again.\")\n",
    "\n",
    "print(f\"You have selected: {selected_model['name']}\")\n",
    "\n",
    "# Dynamically set up the client based on the chosen provider\n",
    "client = None\n",
    "provider = selected_model[\"provider\"]\n",
    "\n",
    "if provider == \"openai\":\n",
    "    from openai import OpenAI\n",
    "    api_key = getpass(\"Please enter your OpenAI API key: \")\n",
    "    client = OpenAI(api_key=api_key)\n",
    "elif provider == \"anthropic\":\n",
    "    from anthropic import Anthropic\n",
    "    api_key = getpass(\"Please enter your Anthropic API key: \")\n",
    "    client = Anthropic(api_key=api_key)\n",
    "elif provider == \"google\":\n",
    "    print(\"Authenticating to Google Cloud via Vertex AI environment...\")\n",
    "    try:\n",
    "        import vertexai\n",
    "        from vertexai.generative_models import GenerativeModel, Part, HarmCategory, HarmBlockThreshold\n",
    "        \n",
    "        PROJECT_ID = \"interviewai-456517\"  \n",
    "        LOCATION = \"us-central1\"          \n",
    "        \n",
    "        vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "        client = GenerativeModel(selected_model['id']) # Instantiate the specific model\n",
    "        print(\"Successfully initialized Gemini model.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR: Could not initialize Vertex AI. Ensure you are in a Vertex AI notebook and your Project ID is correct.\")\n",
    "        print(f\"Details: {e}\")\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c81d1def-4ef9-4f5d-9cc4-3bc53c043c7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 3. Generalized Text Generation Function ---\n",
    "def generate_text(topic, word_count, model_id, provider):\n",
    "    \"\"\"\n",
    "    Generates a passage of text using the dynamically selected model.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are a writing assistant.\n",
    "\n",
    "Write an original passage on the topic: '{topic}'. It should be approximately {word_count} words long. Be clear and human-like. Avoid copying or referencing specific texts.\n",
    "\n",
    "⚠️ Do not include or repeat the topic or instructions in your output. Return only the generated passage text.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        if provider == \"openai\":\n",
    "            response = client.chat.completions.create(\n",
    "                model=model_id,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.7,\n",
    "                max_tokens=min(word_count * 2, 4096)\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "            \n",
    "        elif provider == \"anthropic\":\n",
    "            # Anthropic uses a 'system' prompt and has a different response structure\n",
    "            response = client.messages.create(\n",
    "                model=model_id,\n",
    "                system=\"You are a writing assistant. Your goal is to write clear, human-like, original passages on a given topic. ⚠️ Do not include or repeat the topic or instructions in your output. Return only the generated passage text.\",\n",
    "                messages=[{\"role\": \"user\", \"content\": f\"Write an original passage on the topic: '{topic}'. It should be approximately {word_count} words long.\"}],\n",
    "                temperature=0.7,\n",
    "                max_tokens=min(word_count * 2, 4096)\n",
    "            )\n",
    "            return response.content[0].text.strip()\n",
    "            \n",
    "        elif provider == \"google\":\n",
    "            # The 'client' is the Gemini model instance itself\n",
    "            response = client.generate_content(\n",
    "                [prompt],\n",
    "                generation_config={\"temperature\": 0.7, \"max_output_tokens\": min(word_count * 2, 4096)},\n",
    "                safety_settings={\n",
    "                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "                }\n",
    "            )\n",
    "            return response.text.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Generation error for model {model_id} on topic '{topic[:30]}...': {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d570576-3e07-4aa7-b0ef-fe5923edad7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved to: generated_output_gpt-4.1-2025-04-14.json\n",
      "Resuming from existing file: generated_output_gpt-4.1-2025-04-14.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Generating with GPT-4.1: 100%|██████████| 2100/2100 [00:00<00:00, 1068547.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved. 1992 samples in generated_output_gpt-4.1-2025-04-14.json\n",
      "\n",
      "All done! Newly generated 0 passages.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Main Processing Logic (with updates for dynamic models) ---\n",
    "\n",
    "## PROVIDE OUTPUT FROM TOPIC GENERATION\n",
    "file_path = \"\" \n",
    "\n",
    "# Output path is now generated dynamically to separate results from different models\n",
    "output_path = f\"generated_output_{selected_model['id'].replace('/', '_')}.json\"\n",
    "print(f\"Results will be saved to: {output_path}\")\n",
    "\n",
    "processed_count = 0\n",
    "\n",
    "# Load existing data if the output file for this model already exists\n",
    "if os.path.exists(output_path):\n",
    "    print(f\"Resuming from existing file: {output_path}\")\n",
    "    with open(output_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        processed_data = json.load(f)\n",
    "else:\n",
    "    processed_data = []\n",
    "\n",
    "# Create a lookup for already processed items to avoid duplicates\n",
    "processed_lookup = {item['text']: item for item in processed_data}\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    source_data = json.load(f)\n",
    "\n",
    "def save_current_progress():\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(list(processed_lookup.values()), f, ensure_ascii=False, indent=2)\n",
    "    print(f\"\\nProgress saved. {len(processed_lookup)} samples in {output_path}\")\n",
    "\n",
    "try:\n",
    "    for item in tqdm(source_data, desc=f\"🧠 Generating with {selected_model['name']}\"):\n",
    "        topic = item.get(\"topic\", \"\").strip()\n",
    "        text = item.get(\"text\", \"\").strip()\n",
    "\n",
    "        if not topic or not text:\n",
    "            continue\n",
    "\n",
    "        # Check if this text has been processed before\n",
    "        if text in processed_lookup:\n",
    "            # It exists, now check if it was processed by the *current* model\n",
    "            existing_item = processed_lookup[text]\n",
    "            if \"ai_generated\" not in existing_item:\n",
    "                existing_item[\"ai_generated\"] = {}\n",
    "            \n",
    "            if not existing_item[\"ai_generated\"].get(selected_model['id']):\n",
    "                word_count = len(text.split())\n",
    "                ai_text = generate_text(topic, word_count, selected_model['id'], provider)\n",
    "                if ai_text:\n",
    "                    existing_item[\"ai_generated\"][selected_model['id']] = ai_text\n",
    "                    processed_count += 1\n",
    "                    time.sleep(1) # Be a good citizen\n",
    "        else:\n",
    "            # It's a new item, generate text for it\n",
    "            word_count = len(text.split())\n",
    "            ai_text = generate_text(topic, word_count, selected_model['id'], provider)\n",
    "            if ai_text:\n",
    "                new_item = item.copy() # Avoid modifying the source data list\n",
    "                new_item[\"ai_generated\"] = {selected_model['id']: ai_text}\n",
    "                processed_lookup[text] = new_item\n",
    "                processed_count += 1\n",
    "                time.sleep(1)\n",
    "\n",
    "        # Save progress periodically\n",
    "        if processed_count > 0 and processed_count % 10 == 0:\n",
    "            save_current_progress()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nInterrupted by user. Saving final progress...\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred: {e}\")\n",
    "    save_current_progress()\n",
    "    raise\n",
    "\n",
    "# Final save at the end of the script\n",
    "save_current_progress()\n",
    "print(f\"\\nAll done! Newly generated {processed_count} passages.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2268d3-b546-4262-86a5-48c1c59ea46c",
   "metadata": {},
   "source": [
    "# Use Five AI Detectors (4 API and 1 fine-tuned model) to Detect AI Texts and Provide Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957f72fc-9afb-4d50-86d5-47a788444b6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Pengram API Key:  ········\n",
      "Enter Originality.AI API Key:  ········\n",
      "Enter GPTZero API Key:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API keys loaded for this session.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Running all AI detectors: 100%|██████████| 1992/1992 [4:28:38<00:00,  8.09s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Multi-detector analysis completed.\n",
      "Total items: 1992\n",
      "Skipped items (already processed): 0\n",
      "Processed items: 1992\n",
      "Results saved to: generated_output_claude-opus-4-20250514_all_detectors.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# merge all ai-detector, and remain default prob\n",
    "# Multi-detector AI text analysis with default prob\n",
    "import json\n",
    "import requests\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "from getpass import getpass\n",
    "\n",
    "# === Prompt for API keys securely ===\n",
    "api_keys = {\n",
    "    \"pengram\": getpass(\"Enter Pengram API Key: \"),\n",
    "    \"originality\": getpass(\"Enter Originality.AI API Key: \"),\n",
    "    \"gptzero\": getpass(\"Enter GPTZero API Key: \")\n",
    "}\n",
    "\n",
    "print(\"✅ API keys loaded for this session.\")\n",
    "\n",
    "# === Pangram 检测函数 ===\n",
    "def detect_ai_pangram(text):\n",
    "    if not text.strip():\n",
    "        return None\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"x-api-key\": api_keys[\"pengram\"]\n",
    "    }\n",
    "    payload = {\"text\": text}\n",
    "    try:\n",
    "        response = requests.post(\"https://text.api.pangramlabs.com\", headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "\n",
    "        likelihood = result.get(\"ai_likelihood\")\n",
    "        prediction = result.get(\"prediction\")\n",
    "\n",
    "        return {\n",
    "            \"ai_likelihood\": likelihood,\n",
    "            \"prediction\": prediction\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# === Originality.ai wrapper ===\n",
    "def detect_ai_originality(text):\n",
    "    if not text.strip():\n",
    "        return None\n",
    "    try:\n",
    "        headers = {\n",
    "            \"X-OAI-API-KEY\": api_keys[\"originality\"],\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        payload = {\n",
    "            \"check_ai\": True,\n",
    "            \"check_plagiarism\": False,\n",
    "            \"check_facts\": False,\n",
    "            \"check_readability\": False,\n",
    "            \"check_grammar\": False,\n",
    "            \"check_contentOptimizer\": False,\n",
    "            \"storeScan\": False,\n",
    "            \"aiModelVersion\": \"lite\",\n",
    "            \"content\": text\n",
    "        }\n",
    "        response = requests.post(\"https://api.originality.ai/api/v3/scan\", headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        classification = result.get(\"results\", {}).get(\"ai\", {}).get(\"classification\", {})\n",
    "        confidence = result.get(\"results\", {}).get(\"ai\", {}).get(\"confidence\", {})\n",
    "        return {\n",
    "            \"classification\": {\n",
    "                \"AI\": classification.get(\"AI\"),\n",
    "                \"Original\": classification.get(\"Original\")\n",
    "            },\n",
    "            \"confidence\": {\n",
    "                \"AI\": confidence.get(\"AI\"),\n",
    "                \"Original\": confidence.get(\"Original\")\n",
    "            }\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# === GPTZero detector wrapper ===\n",
    "def detect_ai_gptzero(text):\n",
    "    if not text.strip():\n",
    "        return None\n",
    "    try:\n",
    "        headers = {\n",
    "            \"Accept\": \"application/json\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"x-api-key\": api_keys[\"gptzero\"]\n",
    "        }\n",
    "        payload = {\n",
    "            \"document\": text,\n",
    "            \"multilingual\": False\n",
    "        }\n",
    "        response = requests.post(\n",
    "            \"https://api.gptzero.me/v2/predict/text\",\n",
    "            headers=headers,\n",
    "            json=payload\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        doc = response.json().get(\"documents\", [{}])[0]\n",
    "        return {\n",
    "            \"average_generated_prob\": doc.get(\"average_generated_prob\")\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# === Load RoBERTa-based OpenAI Detector ===\n",
    "MODEL_NAME = \"roberta-base-openai-detector\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "detector = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME).eval()\n",
    "\n",
    "# === RoBERTa detector function ===\n",
    "def detect_ai_roberta(text):\n",
    "    if not text.strip():\n",
    "        return None\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        logits = detector(**inputs).logits\n",
    "    prob_ai = torch.softmax(logits, dim=-1)[0, 1].item()\n",
    "    return prob_ai\n",
    "\n",
    "# === Load input data ===\n",
    "input_path = \"\"\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 统计信息\n",
    "total_items = len(data)\n",
    "skipped_items = 0\n",
    "processed_items = 0\n",
    "\n",
    "# === Run all detectors ===\n",
    "for item in tqdm(data, desc=\"Running all AI detectors\"):\n",
    "    # 检查是否已有检测结果，如果有则跳过\n",
    "    if \"human_verdict\" in item and \"ai_verdicts\" in item and all(model in item[\"ai_verdicts\"] for model in item.get(\"ai_generated\", {})):\n",
    "        skipped_items += 1\n",
    "        continue\n",
    "\n",
    "    human_text = item.get(\"text\", \"\")\n",
    "\n",
    "    # Initialize verdict structure if needed\n",
    "    item.setdefault(\"human_verdict\", {})\n",
    "\n",
    "    # Add results from each detector\n",
    "    item[\"human_verdict\"][\"pengram\"] = detect_ai_pangram(human_text)\n",
    "    item[\"human_verdict\"][\"originality\"] = detect_ai_originality(human_text)\n",
    "    item[\"human_verdict\"][\"gptzero\"] = detect_ai_gptzero(human_text)\n",
    "    item[\"human_verdict\"][\"roberta-base-detector\"] = detect_ai_roberta(human_text)\n",
    "\n",
    "    # Process AI-generated texts\n",
    "    ai_texts = item.get(\"ai_generated\", {})\n",
    "    item.setdefault(\"ai_verdicts\", {})\n",
    "\n",
    "    for model_name, text in ai_texts.items():\n",
    "        item[\"ai_verdicts\"].setdefault(model_name, {})\n",
    "        item[\"ai_verdicts\"][model_name][\"pengram\"] = detect_ai_pangram(text)\n",
    "        item[\"ai_verdicts\"][model_name][\"originality\"] = detect_ai_originality(text)\n",
    "        item[\"ai_verdicts\"][model_name][\"gptzero\"] = detect_ai_gptzero(text)\n",
    "        item[\"ai_verdicts\"][model_name][\"roberta-base-detector\"] = detect_ai_roberta(text)\n",
    "\n",
    "    processed_items += 1\n",
    "\n",
    "# === Save output ===\n",
    "output_path = input_path.replace(\".json\", \"_all_detectors.json\")\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ Multi-detector analysis completed.\")\n",
    "print(f\"Total items: {total_items}\")\n",
    "print(f\"Skipped items (already processed): {skipped_items}\")\n",
    "print(f\"Processed items: {processed_items}\")\n",
    "print(f\"Results saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f19da65-bd92-47c1-89e8-d146f9de8d82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
